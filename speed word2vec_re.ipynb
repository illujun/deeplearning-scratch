{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49951d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output:\n",
      "tensor([[ 3.,  4.,  5.],\n",
      "        [ 0.,  1.,  2.],\n",
      "        [ 9., 10., 11.],\n",
      "        [ 0.,  1.,  2.]], grad_fn=<IndexBackward0>)\n",
      "Gradient of W:\n",
      "tensor([[2., 2., 2.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "W = torch.tensor(W, dtype=torch.float32, requires_grad=True)  # requires_grad=True는 기울기 계산을 위한 설정\n",
    "\n",
    "# 인덱스를 정의\n",
    "idx = torch.tensor([1, 0, 3, 0])\n",
    "\n",
    "# Embedding 클래스 구현\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [torch.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        self.idx = idx\n",
    "        W, = self.params\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW.zero_()  # 기울기를 0으로 초기화\n",
    "        # dout을 인덱스에 맞게 더함\n",
    "        for i, idx in enumerate(self.idx):\n",
    "            dW[idx] += dout[i]\n",
    "        return None\n",
    "\n",
    "# 모델 초기화\n",
    "embedding = Embedding(W)\n",
    "\n",
    "# 순전파 (forward)\n",
    "out = embedding.forward(idx)\n",
    "print(\"Forward output:\")\n",
    "print(out)\n",
    "\n",
    "# 임의의 기울기 (dout)\n",
    "dout = torch.ones_like(out)\n",
    "\n",
    "# 역전파 (backward)\n",
    "embedding.backward(dout)\n",
    "print(\"Gradient of W:\")\n",
    "print(embedding.grads[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da64de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        # Embedding 클래스는 nn.Embedding으로 구현\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(W, dtype=torch.float32))\n",
    "        self.params = self.embed.parameters()  # 가중치 파라미터\n",
    "        self.grads = [torch.zeros_like(param) for param in self.params]  # 기울기 초기화\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        # target_W를 가져오는 과정\n",
    "        target_W = self.embed(idx)\n",
    "        out = torch.sum(target_W * h, dim=1)  # 내적 계산\n",
    "        \n",
    "        # 캐시 저장 (h, target_W)\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # 역전파 계산\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.view(-1, 1)  # dout 크기 변경\n",
    "        \n",
    "        # 기울기 계산\n",
    "        dtarget_W = dout * h\n",
    "        target_W.grad = dtarget_W.sum(dim=0)  # accumulate gradient\n",
    "        \n",
    "        dh = dout * target_W\n",
    "        return dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579e8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "['say', 'hello', '.', 'goodbye', 'i']\n",
      "you\n",
      "tensor([0.6420, 0.3315, 0.0265])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 단어 목록\n",
    "words = ['you', 'say', 'goodbye', 'i', 'hello', '.']\n",
    "\n",
    "# 무작위로 하나의 단어 선택 (torch.randint로 처리)\n",
    "random_idx = torch.randint(0, len(words), (1,))\n",
    "print(words[random_idx])\n",
    "\n",
    "# 무작위로 5개 단어 선택 (중복 없이)\n",
    "indices = torch.randperm(len(words))[:5]\n",
    "print([words[i] for i in indices])\n",
    "\n",
    "# 확률 분포에 따른 무작위 선택\n",
    "p = torch.tensor([0.5, 0.1, 0.05, 0.2, 0.05, 0.1])\n",
    "chosen_idx = torch.multinomial(p, 1)\n",
    "print(words[chosen_idx])\n",
    "\n",
    "# 확률 분포 변경\n",
    "p = torch.tensor([0.7, 0.29, 0.01])\n",
    "new_p = torch.pow(p, 0.75)\n",
    "\n",
    "# 확률을 합이 1이 되도록 정규화\n",
    "new_p = new_p / new_p.sum()\n",
    "print(new_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8f2809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [2, 0],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Negative Sampling 클래스 구현 (PyTorch 버전)\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size, device='cpu'):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "        self.device = device\n",
    "        \n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "            \n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.word_p = torch.zeros(vocab_size, dtype=torch.float32, device=self.device)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "            \n",
    "        self.word_p = self.word_p ** power\n",
    "        self.word_p /= self.word_p.sum()  # 정규화\n",
    "        \n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "        \n",
    "        negative_sample = torch.zeros((batch_size, self.sample_size), dtype=torch.int64, device=self.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            p = self.word_p.clone()\n",
    "            target_idx = target[i]\n",
    "            p[target_idx] = 0  # target이 뽑히지 않도록 함\n",
    "            p /= p.sum()  # 정규화\n",
    "            \n",
    "            # 음성 샘플링: multinomial 사용\n",
    "            negative_sample[i, :] = torch.multinomial(p, self.sample_size, replacement=False)\n",
    "            \n",
    "        return negative_sample\n",
    "\n",
    "\n",
    "# 예시 데이터\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])  # 샘플 corpus\n",
    "power = 0.75  # power\n",
    "sample_size = 2  # 음성 샘플의 수\n",
    "\n",
    "# `UnigramSampler` 인스턴스 생성\n",
    "sampler = UnigramSampler(corpus, power, sample_size, device='cpu')\n",
    "\n",
    "# target 샘플\n",
    "target = torch.tensor([1, 3, 0], dtype=torch.int64, device='cpu')\n",
    "\n",
    "# 음성 샘플링\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93dba3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        super().__init__()\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        \n",
    "        # BCEWithLogitsLoss는 시그모이드와 BCELoss를 결합한 함수\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embed = nn.Embedding.from_pretrained(W, freeze=False)\n",
    "        \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        # 긍정적 예 순전파\n",
    "        positive_score = torch.sum(self.embed(target) * h, dim=1)\n",
    "        correct_label = torch.ones(batch_size, device=h.device)\n",
    "        loss = self.loss_fn(positive_score, correct_label)\n",
    "        \n",
    "        # 부정적 예 순전파\n",
    "        negative_label = torch.zeros(batch_size, device=h.device)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            negative_score = torch.sum(self.embed(negative_target) * h, dim=1)\n",
    "            loss += self.loss_fn(negative_score, negative_label)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        # backward는 자동으로 PyTorch에서 처리됩니다.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c37432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus, sample_size=5, power=0.75):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.in_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Negative Sampling 손실 함수\n",
    "        self.ns_loss = NegativeSamplingLoss(self.out_embedding.weight, corpus, power=power, sample_size=sample_size)\n",
    "        \n",
    "        # 모델 파라미터\n",
    "        self.params = list(self.in_embedding.parameters()) + list(self.out_embedding.parameters())\n",
    "        self.grads = list(self.in_embedding.parameters()) + list(self.out_embedding.parameters())\n",
    "        \n",
    "    def forward(self, contexts, target):\n",
    "        # 컨텍스트 단어들에 대한 평균 임베딩 계산\n",
    "        h = self.in_embedding(contexts)  # (batch_size, window_size * 2, hidden_size)\n",
    "        h = h.mean(dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # NegativeSamplingLoss를 통해 손실 계산\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout /= len(self.in_embedding.weight)\n",
    "        # Embedding Layer의 역전파\n",
    "        self.in_embedding.weight.grad += dout\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cacf6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus, sample_size=5, power=0.75):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.in_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Negative Sampling 손실 함수\n",
    "        self.ns_loss = NegativeSamplingLoss(self.out_embedding.weight, corpus, power=power, sample_size=sample_size)\n",
    "        \n",
    "        # 모델 파라미터\n",
    "        self.params = list(self.in_embedding.parameters()) + list(self.out_embedding.parameters())\n",
    "        self.grads = list(self.in_embedding.parameters()) + list(self.out_embedding.parameters())\n",
    "        \n",
    "    def forward(self, contexts, target):\n",
    "        # 컨텍스트 단어들에 대한 평균 임베딩 계산\n",
    "        h = self.in_embedding(contexts)  # (batch_size, window_size * 2, hidden_size)\n",
    "        h = h.mean(dim=1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # NegativeSamplingLoss를 통해 손실 계산\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout /= len(self.in_embedding.weight)\n",
    "        # Embedding Layer의 역전파\n",
    "        self.in_embedding.weight.grad += dout\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02aa2a4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# 훈련 시작\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# 훈련 과정 시각화\u001b[39;00m\n\u001b[32m     70\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(max_epoch), total_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data_loader, optimizer, loss_fn, max_epoch)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# 역전파 및 최적화\u001b[39;00m\n\u001b[32m     61\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:466\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    462\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001b[32m    463\u001b[39m             grad, grad, value=cast(\u001b[38;5;28mfloat\u001b[39m, \u001b[32m1\u001b[39m - beta2)\n\u001b[32m    464\u001b[39m         )\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    469\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from dataset import ptb\n",
    "from common.util import create_contexts_target\n",
    "from torch import nn, optim\n",
    "\n",
    "# 파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 1\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# 데이터 준비\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "\n",
    "# PyTorch Dataset 및 DataLoader\n",
    "contexts_tensor = torch.tensor(contexts, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "dataset = TensorDataset(contexts_tensor, target_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 모델 정의 (SkipGram)\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.in_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_embedding(contexts)\n",
    "        h = h.mean(dim=1)  # 컨텍스트 단어들의 평균을 구합니다.\n",
    "        score = torch.sum(self.out_embedding(target) * h, dim=1)\n",
    "        return score\n",
    "\n",
    "# 모델 초기화\n",
    "model = SkipGram(vocab_size, hidden_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "# 훈련 루프\n",
    "def train(model, data_loader, optimizer, loss_fn, max_epoch):\n",
    "    model.train()\n",
    "    for epoch in range(max_epoch):\n",
    "        total_loss = 0\n",
    "        for contexts, target in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 모델 출력 및 손실 계산\n",
    "            score = model(contexts, target)\n",
    "            correct_label = torch.ones_like(score)\n",
    "            loss = loss_fn(score, correct_label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{max_epoch}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 훈련 시작\n",
    "train(model, data_loader, optimizer, loss_fn, max_epoch)\n",
    "\n",
    "# 훈련 과정 시각화\n",
    "plt.plot(range(max_epoch), total_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615b1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
