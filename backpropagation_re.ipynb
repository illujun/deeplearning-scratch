{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a1c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.0\n",
      "dapple     = 2.200000047683716\n",
      "dapple_num = 110.0\n",
      "dtax       = 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkdwn\\AppData\\Local\\Temp\\ipykernel_17096\\1173685861.py:8: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
      "  print(float(price))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "apple      = torch.tensor(100.0, requires_grad=True)  \n",
    "apple_num  = torch.tensor(2.0,   requires_grad=True)\n",
    "tax        = torch.tensor(1.1,   requires_grad=True)\n",
    "\n",
    "price = apple * apple_num * tax  \n",
    "print(float(price))  \n",
    "\n",
    "\n",
    "price.backward() \n",
    "\n",
    "print(\"dapple     =\", apple.grad.item())      \n",
    "print(\"dapple_num =\", apple_num.grad.item())  \n",
    "print(\"dtax       =\", tax.grad.item())        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef26796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price = 715.0\n",
      "dapple      = 2.200000047683716\n",
      "dapple_num  = 110.0\n",
      "dorange     = 3.3000001907348633\n",
      "dorange_num = 165.0\n",
      "dtax        = 650.0\n"
     ]
    }
   ],
   "source": [
    "apple       = torch.tensor(100.0, requires_grad=True)\n",
    "apple_num   = torch.tensor(2.0,   requires_grad=True)\n",
    "orange      = torch.tensor(150.0, requires_grad=True)\n",
    "orange_num  = torch.tensor(3.0,   requires_grad=True)\n",
    "tax         = torch.tensor(1.1,   requires_grad=True)\n",
    "\n",
    "apple_price   = apple * apple_num      \n",
    "orange_price  = orange * orange_num    \n",
    "all_price     = apple_price + orange_price \n",
    "price         = all_price * tax         \n",
    "\n",
    "print(\"price =\", float(price)) \n",
    "\n",
    "price.backward()\n",
    "\n",
    "print(\"dapple      =\", apple.grad.item())      \n",
    "print(\"dapple_num  =\", apple_num.grad.item()) \n",
    "print(\"dorange     =\", orange.grad.item())     \n",
    "print(\"dorange_num =\", orange_num.grad.item())  \n",
    "print(\"dtax        =\", tax.grad.item())       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65209627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ReLU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.relu(x)\n",
    "    \n",
    "class Sigmoid(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78daa3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      " tensor([[ 1.0000, -0.5000],\n",
      "        [-2.0000,  3.0000]])\n",
      "mask(x<=0) =\n",
      " tensor([[False,  True],\n",
      "        [ True, False]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0, -0.5],\n",
    "                  [-2.0,  3.0]])\n",
    "\n",
    "print(\"x =\\n\", x)\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(\"mask(x<=0) =\\n\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41a2830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(x) =\n",
      " tensor([[1., 0.],\n",
      "        [0., 3.]])\n",
      "Sigmoid(x) =\n",
      " tensor([[0.7311, 0.3775],\n",
      "        [0.1192, 0.9526]])\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "sigm = Sigmoid()\n",
    "\n",
    "# 순전파\n",
    "y_relu = relu(x)         # ReLU\n",
    "y_sigm = sigm(x)         # Sigmoid\n",
    "\n",
    "print(\"ReLU(x) =\\n\", y_relu)\n",
    "print(\"Sigmoid(x) =\\n\", y_sigm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67543c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) torch.Size([2, 3]) torch.Size([3])\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [10., 10., 10.]])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [11., 12., 13.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(2)\n",
    "W = torch.rand(2, 3)\n",
    "B = torch.rand(3)\n",
    "\n",
    "print(X.shape, W.shape, B.shape)\n",
    "\n",
    "Y = X @ W + B\n",
    "\n",
    "X_dot_W = torch.tensor([[0., 0., 0.], [10., 10., 10.]])\n",
    "B = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)\n",
    "\n",
    "dY = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(dY)\n",
    "\n",
    "dB = dY.sum(dim=0)\n",
    "print(dB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self, W: torch.Tensor, b: torch.Tensor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = nn.Parameter(W.clone().detach().float())\n",
    "        self.b = nn.Parameter(b.clone().detach().float())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "class SoftmaxWithLoss(nn.Module):\n",
    "    def __init__(self, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        if t.dim() == 2 and t.shape == x.shape:\n",
    "            log_probs = F.log_softmax(x, dim=1)  \n",
    "            loss_vec = -(t * log_probs).sum(dim=1)   \n",
    "            if self.reduction == \"mean\":\n",
    "                return loss_vec.mean()\n",
    "            elif self.reduction == \"sum\":\n",
    "                return loss_vec.sum()\n",
    "            else:\n",
    "                return loss_vec  # 'none'\n",
    "        else:\n",
    "            # (2) t가 class index인 경우: PyTorch의 cross_entropy 사용\n",
    "            # 기존 backward의\n",
    "            #   if one-hot: dx = (y - t) / N\n",
    "            #   else: dx = (y - one_hot(t)) / N\n",
    "            # 를 autograd가 내부적으로 처리합니다.\n",
    "            return F.cross_entropy(x, t, reduction=self.reduction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb052267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'common'))\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01, device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        W1 = weight_init_std * torch.randn(input_size, hidden_size, device=self.device)\n",
    "        b1 = torch.zeros(hidden_size, device=self.device)\n",
    "        W2 = weight_init_std * torch.randn(hidden_size, output_size, device=self.device)\n",
    "        b2 = torch.zeros(output_size, device=self.device)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(W1, b1).to(self.device)\n",
    "        self.layers['Relu1']   = relu().to(self.device)\n",
    "        self.layers['Affine2'] = Affine(W2, b2).to(self.device)\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss().to(self.device)\n",
    "        self.params = {\n",
    "            'W1': self.layers['Affine1'].W,\n",
    "            'b1': self.layers['Affine1'].b,\n",
    "            'W2': self.layers['Affine2'].W,\n",
    "            'b2': self.layers['Affine2'].b,\n",
    "        }\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        return x.to(self.device).float()\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self._to_tensor(x)\n",
    "        for layer in self.layers.values():\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        x = self._to_tensor(x)\n",
    "        t = self._to_tensor(t)\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        x = self._to_tensor(x)\n",
    "        t = self._to_tensor(t)\n",
    "        y = self.predict(x).argmax(dim=1)\n",
    "        if t.dim() != 1:\n",
    "            t = t.argmax(dim=1)\n",
    "        return (y == t).float().mean().item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def numerical_gradient(self, x, t, eps=1e-4):\n",
    "        x_t = self._to_tensor(x)\n",
    "        t_t = self._to_tensor(t)\n",
    "        grads = {}\n",
    "\n",
    "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "            p = self.params[key]\n",
    "            grad = torch.zeros_like(p)\n",
    "            it = np.nditer(p.detach().cpu().numpy(), flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                idx = it.multi_index\n",
    "                old = p[idx].item()\n",
    "\n",
    "                p.data[idx] = old + eps\n",
    "                l1 = self.loss(x_t, t_t).item()\n",
    "\n",
    "                p.data[idx] = old - eps\n",
    "                l2 = self.loss(x_t, t_t).item()\n",
    "\n",
    "                grad[idx] = (l1 - l2) / (2 * eps)\n",
    "                p.data[idx] = old\n",
    "                it.iternext()\n",
    "\n",
    "            grads[key] = grad.to(self.device)\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # autograd로 역전파 (원본의 backward 순서를 내부적으로 대체)\n",
    "        for p in self.params.values():\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "        loss = self.loss(x, t)\n",
    "        loss.backward()\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.params['W1'].grad.detach().clone(),\n",
    "            'b1': self.params['b1'].grad.detach().clone(),\n",
    "            'W2': self.params['W2'].grad.detach().clone(),\n",
    "            'b2': self.params['b2'].grad.detach().clone(),\n",
    "        }\n",
    "\n",
    "        # 원본과의 호환: Affine 레이어에도 dW/db 필드 채워두기\n",
    "        self.layers['Affine1'].dW = grads['W1']\n",
    "        self.layers['Affine1'].db = grads['b1']\n",
    "        self.layers['Affine2'].dW = grads['W2']\n",
    "        self.layers['Affine2'].db = grads['b2']\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08aef940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:1.3743592392214974e-06\n",
      "b1:2.103025242342722e-05\n",
      "W2:4.907778803998354e-09\n",
      "b2:1.3919942052492695e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'dataset'))\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a81926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10863333333333333, 0.1056\n",
      "train acc, test acc | 0.9072666666666667, 0.9093\n",
      "train acc, test acc | 0.92115, 0.924\n",
      "train acc, test acc | 0.9384833333333333, 0.938\n",
      "train acc, test acc | 0.9459666666666666, 0.9433\n",
      "train acc, test acc | 0.9518666666666666, 0.9486\n",
      "train acc, test acc | 0.9572166666666667, 0.9548\n",
      "train acc, test acc | 0.9612166666666667, 0.9578\n",
      "train acc, test acc | 0.9638833333333333, 0.9596\n",
      "train acc, test acc | 0.9681833333333333, 0.9643\n",
      "train acc, test acc | 0.9701333333333333, 0.9641\n",
      "train acc, test acc | 0.9715333333333334, 0.9661\n",
      "train acc, test acc | 0.97375, 0.9682\n",
      "train acc, test acc | 0.9752333333333333, 0.9673\n",
      "train acc, test acc | 0.9736333333333334, 0.9666\n",
      "train acc, test acc | 0.9774833333333334, 0.9699\n",
      "train acc, test acc | 0.9789833333333333, 0.9708\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'dataset'))\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet  # ← NumPy 버전\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list, train_acc_list, test_acc_list = [], [], []\n",
    "iter_per_epoch = max(train_size // batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * learning_rate * 0 + learning_rate * (-0)  # placeholder\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "        #twolayernet이 pytorch 기반이 아니라서 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38aef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
